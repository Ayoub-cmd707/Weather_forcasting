{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce09be5-241e-49af-86ec-b563668577c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cf2cdm\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "class EUPPFullEnsembleDataset(Dataset):\n",
    "    def __init__(self, nsample, target_var,data_path=\"data\",num_ensemble=11, dataset_type=\"train\", normalized=True, return_time=False):\n",
    "        self.num_ensemble = num_ensemble\n",
    "        self.normalized = normalized\n",
    "        self.return_time = return_time\n",
    "        self.data_path = data_path\n",
    "        if target_var == \"t2m\":\n",
    "            self.variables = ['t2m', 'z', 't', 'u10', 'v10', 'tcc', 'sd', 'mx2t6', 'mn2t6', 'w10', 'p10fg6', 'oro'] #12\n",
    "        elif target_var == \"w10\":\n",
    "            self.variables = ['t2m', 'z', 't', 'u10', 'v10', 'tcc', 'sd', 'mx2t6', 'mn2t6', 'w10', 'u100', 'w100', 'p10fg6', 'v100', 'oro'] #15\n",
    "        elif target_var == \"w100\":\n",
    "            self.variables = ['t2m', 'z', 't', 'u10', 'v10', 'tcc', 'w10', 'u100', 'w100', 'u', 'w700', 'p10fg6', 'v100', 'v', 'oro'] #15\n",
    "        else:\n",
    "            self.variables = []  \n",
    "        self.target_var = target_var\n",
    "        self.value_range = {\"t2m\":(235, 304), \"z\": (48200, 58000), \"t\":(240, 299), \"u10\": (-13., 11.),\"v10\": (-30,35), \"tcc\": (0., 1.0),\"sd\":(0,8),\"mx2t6\":(230,320),\"mn2t6\":(225,315),\"v\":(-50,55), \"w100\":(0,50),\"w10\":(0,30), \"u100\": (-35,45), \"u\": (-45,60),\"v100\":(-40,45), \"w700\": (0,60), \"p10fg6\": (0,60), \"oro\":(-400,2800)}\n",
    "\n",
    "        self.train_eupp_files = []\n",
    "        self.train_era5_files = []\n",
    "        self.val_eupp_files = []\n",
    "        self.val_era5_files = []\n",
    "        self.test_eupp_files = []\n",
    "        self.test_era5_files = []\n",
    "    \n",
    "            \n",
    "        eupp_files = glob.glob(\"./data/EUPP/output.sfc.*.nc\")\n",
    "        era5_files = glob.glob(\"/data/ERA5/era.sfc.*.nc\")\n",
    "        \n",
    "        eupp_file_train_path = \"./TrainValTestSplit/train_eupp_files.pkl\"\n",
    "        eupp_file_val_path = \"./TrainValTestSplit/val_eupp_files.pkl\"\n",
    "        era5_file_train_path = \"./TrainValTestSplit/train_era5_files.pkl\"\n",
    "        era5_file_val_path = \"./TrainValTestSplit/val_era5_files.pkl\"\n",
    "  \n",
    "        # Load the split files based on dataset type\n",
    "        if dataset_type == \"train\":\n",
    "            with open(eupp_file_train_path, 'rb') as f:\n",
    "                self.eupp_files = pickle.load(f)\n",
    "            with open(era5_file_train_path, 'rb') as f:\n",
    "                self.era5_files = pickle.load(f)\n",
    "        elif dataset_type == \"test\":\n",
    "            with open(eupp_file_val_path, 'rb') as f:\n",
    "                self.eupp_files = pickle.load(f)\n",
    "            with open(era5_file_val_path, 'rb') as f:\n",
    "                self.era5_files = pickle.load(f)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.eupp_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            ds_eupp = xr.open_dataset(self.eupp_files[idx]).drop_vars(\"time\", errors=\"ignore\")\n",
    "            ds_eupp = ds_eupp.fillna(9999.0)\n",
    "            ds_era5 = xr.open_dataset(self.era5_files[idx]).fillna(9999.0).isel(step=slice(1, None))  # Exclude the first step, its absent in the reforecast files \n",
    "            ds_era5 = ds_era5.rename({'w100_obs': 'w100'})\n",
    "            orography_data = xr.open_dataset(\"/home/jupyter-aaron/Postprocessing/TfMBM_wind/baselines/data/oro.nc\") \n",
    "            #orography_data=orography_data.sel(latitude=slice(min_lat, max_lat), longitude=slice(min_lon, max_lon))\n",
    "            ensemble=ds_eupp[\"number\"].values\n",
    "            step=ds_eupp[\"step\"].values\n",
    "            oro_expand=orography_data[\"oro\"].expand_dims(number=ensemble,step=step)\n",
    "            orography_expanded=oro_expand.reindex(number=ensemble, step=step, latitude=orography_data.latitude, longitude=orography_data.longitude)\n",
    "            min_lat, max_lat = 53.5, 45.75\n",
    "            min_lon, max_lon = 2.4, 10.6\n",
    "            ds_eupp = ds_eupp.sel(latitude=slice(min_lat, max_lat), longitude=slice(min_lon, max_lon))\n",
    "            ds_era5 = ds_era5.sel(latitude=slice(min_lat, max_lat), longitude=slice(min_lon, max_lon))\n",
    "            orography_data = orography_data.sel(latitude=slice(min_lat, max_lat), longitude=slice(min_lon, max_lon))\n",
    "             # Load orography data\n",
    "            # Expand orography data to match the dimensions of other variables\n",
    "            # orography_expanded = orography_data[\"oro\"].expand_dims(dim={\"number\": np.arange(self.num_ensemble)})\n",
    "            self.orography_values = torch.as_tensor(np.copy(orography_expanded.to_numpy()))\n",
    "            len_lat=len(ds_eupp['latitude'].values)\n",
    "            len_lon=len(ds_eupp['longitude'].values)\n",
    "            len_TD=len(ds_eupp['step'].values)\n",
    "            ds_eupp = ds_eupp.stack(space=[\"latitude\",\"longitude\"])\n",
    "            ds_era5 = ds_era5.stack(space=[\"latitude\",\"longitude\"])\n",
    "            inputs = torch.zeros((len(self.variables), self.num_ensemble, len_TD,len_lat, len_lon)) #(3,51,32,33) \n",
    "            for k in range(len(self.variables)): #len is 1\n",
    "                variable = self.variables[k] \n",
    "                if variable == 'oro': # Check if variable is orography\n",
    "                    values = self.orography_values # Use pre-loaded orography values\n",
    "                else:\n",
    "                    values = torch.reshape(torch.as_tensor(ds_eupp[variable].to_numpy()[:self.num_ensemble,:]), (self.num_ensemble,len_TD,len_lat,len_lon)) #(11,32,33)\n",
    "                if self.normalized:\n",
    "                    minval, maxval = self.value_range[variable]\n",
    "                    values = (values - minval) / (maxval - minval)\n",
    "                inputs[k, :] = values\n",
    "                if (variable == self.target_var):\n",
    "                    values_tar = torch.reshape(torch.as_tensor(ds_eupp[variable].to_numpy()[:self.num_ensemble,:]), (1,self.num_ensemble,len_TD,len_lat*len_lon))\n",
    "                    targets = torch.reshape(torch.as_tensor(ds_era5[variable].to_numpy()),(1,len_TD,len_lat*len_lon))\n",
    "                    scale_std, scale_mean = torch.std_mean(values_tar, dim=1, unbiased=False)\n",
    "                    \n",
    "            inputs = inputs.movedim(0,1) #(11,1,32,33) \n",
    "            inputs = inputs.movedim(1,-1)\n",
    "\n",
    "\n",
    "            if self.return_time:\n",
    "                return  inputs, targets , scale_mean, scale_std\n",
    "            else:\n",
    "                return inputs, targets , scale_mean, scale_std\n",
    "\n",
    "\n",
    "def loader_prepare(args):\n",
    "    trainloader = DataLoader(EUPPFullEnsembleDataset(data_path=args.data_path,\n",
    "                                                      nsample=32*33,\n",
    "                                                      target_var=args.target_var,\n",
    "                                                      dataset_type='train', num_ensemble=args.ens_num),\n",
    "                                 args.batch_size, shuffle=True, num_workers=8, pin_memory=True, prefetch_factor=6, persistent_workers=True)\n",
    "\n",
    "    testloader = DataLoader(EUPPFullEnsembleDataset(data_path=args.data_path,\n",
    "                                                     nsample=32*33,\n",
    "                                                     target_var=args.target_var,\n",
    "                                                     dataset_type='test', num_ensemble=args.ens_num, return_time=True),\n",
    "                                args.batch_size, shuffle=False, num_workers=4, pin_memory=True, prefetch_factor=4, persistent_workers=False)\n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
